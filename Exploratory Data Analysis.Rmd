---
title: "Exploratory Data Analysis"
output: github_document
date: "2024-10-03"
---

```{r setup, include=FALSE}
library(tidyverse)
library(haven)
```

All of the looking at the data, poking at it between importing and analysis 
Includes: visualisation, checks hypothesis generating (sometimes), mostly about getting a sense of the data before getting started 

Group level summaries today: 
May want to do examine groups visually and quantitatively (can do numeric summaries today)
  Computing a mean, median, sd, missingness in a group 
  
Unless makes the group explicit it doesn't exist especially after you start mutating it etc. group_by makes the grouping that we care about in the dataset explicit so that when we use other things in tidyverse they are aware of it. 

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728", "USW00022534", "USS0023B17S"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2021-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = case_match(
      id, 
      "USW00094728" ~ "CentralPark_NY", 
      "USW00022534" ~ "Molokai_HI",
      "USS0023B17S" ~ "Waterhole_WA"),
    tmin = tmin / 10,
    tmax = tmax / 10,
    month = lubridate::floor_date(date, unit = "month")) |>
  select(name, id, everything())
```

lubridate to use data and time variables (floor_date can round down, here to the first of the month)

Let's make some plots

```{r}
weather_df %>% 
  ggplot(aes(x = prcp)) + 
  geom_histogram()
```

```{r}
weather_df %>% 
  filter(prcp > 1000)
```

Taking a look at stuff and trying to understand what is going on 

```{r}
weather_df %>%
  filter(tmax > 20, tmax < 30) %>% 
  ggplot(aes(x = tmin, y = tmax, color = name, shape = name)) +
  geom_point()
```

something is weird since Central park and Molokai are recording data differently since their data points are appearing in bands where as Waterhole are more scattered 

##group_by()

```{r}
weather_df %>% 
  group_by(name)
```

```{r}
weather_df %>% 
  group_by(month)
```

Not modifying the dataset, but adding a layer ontop for useful subsequent things 

```{r}
weather_df %>% 
  group_by(name) %>% 
  summarise(n_obs = n())
```

```{r}
weather_df %>% 
  group_by(month) %>% 
  summarise(n_obs = n())
```

the number of distinct observations 

```{r}
weather_df %>% 
  group_by(month) %>% 
  summarise(
    n_obs = n(),
    n_dist = n_distinct(month))
```

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  summarise(
    n_obs = n())
```

One off function that does counting 

```{r}
weather_df %>% 
  count(name)
```

##2x2 tables (kinda)

Binary predictor for something to look like a 2x2 table

```{r}
weather_df %>% 
  drop_na(tmax) %>% 
  filter(name!= "Molokai_HI") %>% 
  mutate(
    cold = case_when(
      tmax < 5 ~ "cold", 
      tmax >= 5 ~ "not_cold"
    )
  ) %>% 
  janitor::tabyl(name, cold)
```

##general numeric summaries 

let's try some other useful summaries 

```{r}
weather_df %>% 
  group_by(name) %>% 
  summarise(
    mean_tmax = mean(tmax, na.rm = TRUE),
    median_tmin = median(tmin, na.rm = TRUE),
    sd_prcp = sd(prcp, na.rm = TRUE)
  )
```

After group_by it keeps those groups, be aware of it and will keep thsoe groups as you continue to do other things 

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  summarise(
    mean_tmax = mean(tmax, na.rm = TRUE),
    median_tmin = median(tmin, na.rm = TRUE),
    sd_prcp = sd(prcp, na.rm = TRUE)
  ) %>% 
  ggplot(aes(x = month, y = mean_tmax, color = name)) + 
  geom_point() +
  geom_line()
```

Can be easier to see the trends if do some summary before 
Not looking at all the data all at once

can print the data frames but can also format better for readers 

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  summarise(
    mean_tmax = mean(tmax, na.rm = TRUE)
  )
```

currently hard to read, so making something 'untidy' for easier legibility

```{r}
weather_df %>% 
  group_by(name, month) %>% 
  summarise(
    mean_tmax = mean(tmax, na.rm = TRUE)
  ) %>% 
  pivot_wider(
    names_from = name, 
    values_from = mean_tmax
  ) %>% 
  knitr::kable(
    digits = 3,
    col.names = c("Month", "Central Park", "Molokai", "Waterhole"))
```

digits = 3, means how many numbers after the decimal point
the col.names just makes the names prettier, wihtout will just keep the origianl input name from the name column 

Computing groups and summaries are important in getting to know the dataset 

Other things that we can do ontop of the group

##grouped mutate 

```{r}
weather_df %>% 
  mutate(mean_tmax = mean(tmax, na.rm = TRUE))
```

Here it does the mean tmax of all the observations
But if want mean tmax of specific places (group) then can group by 
Once added grouping layer, then mutate will keep track of those groups 

```{r}
weather_df %>% 
  group_by(name) %>% 
  mutate(
    mean_tmax = mean(tmax, na.rm = TRUE),
    centered_tmax = tmax - mean_tmax) %>% 
  ggplot(aes(x = date, y = centered_tmax, color = name)) +
  geom_point()
```

Centering is each done for them seperately 

##Window function 

The previous example used mean() to compute the mean within each group, which was then subtracted from the observed max tempurature. mean() takes n inputs and produces a single output.

Window functions, in contrast, take n inputs and return n outputs, and the outputs depend on all the inputs. 

Window functions are iterative(?)

Find hottest / coldest days

```{r}
weather_df %>% 
  mutate(
    temp_rank = min_rank(tmax)
  ) %>% 
  filter(temp_rank < 10)
```

ten of the coldest day in the dataframe 

```{r}
weather_df %>% 
  group_by(name) %>% 
  mutate(
    temp_rank = min_rank(tmax)
  ) %>% 
  filter(temp_rank < 4)
```

Now getting top 3 coldest days for each name 

```{r}
weather_df %>% 
  group_by(name) %>% 
  mutate(
    temp_rank = min_rank(desc(tmax))
  ) %>% 
  filter(temp_rank < 4)
```

```{r}
weather_df %>% 
  group_by(name) %>% 
  filter(min_rank(tmax) < 4) %>% 
  arrange(tmax)
```

instead of creating a temperature rank variable just filtering immediately. 
In both of these, we’ve skipped a mutate() statement that would create a ranking variable, and gone straight to filtering based on the result.


```{r}
weather_df %>% 
  group_by(name) %>% 
  mutate(
    lagged_temp = lag(tmax)
  )
```

lagged: compare an observation to it’s previous value. 
This is useful, for example, to find the day-by-day change in max temperature within each station over the year. 
Not that it looks specifically at the sequential date but just looking at sequential rows (will be an issue if there are missing dates, not chronological dates)

```{r}
weather_df %>% 
  group_by(name) %>% 
  mutate(
    lagged_temp = lag(tmax),
    temp_change = tmax - lagged_temp
  ) %>% 
  filter(min_rank(temp_change) < 3)
```

```{r}
weather_df %>% 
  group_by(name) %>% 
  mutate(
    lagged_temp = lag(tmax),
    temp_change = tmax - lagged_temp
  ) %>% 
  summarise(
    sd_tmax_change = sd(temp_change, na.rm = TRUE)
  )
```

##Learning Assessment 

In the PULSE data, the primary outcome is BDI score; it’s observed over follow-up visits, and we might ask if the typical BDI score values are roughly similar at each. Try to write a code chunk that imports, cleans, and summarizes the PULSE data to examine the mean and median at each visit. Export the results of this in a reader-friendly format.

```{r}
pulse_data = 
  haven::read_sas("./data/public_pulse_data.sas7bdat")  %>% 
  janitor::clean_names()  %>% 
  pivot_longer(
    bdi_score_bl:bdi_score_12m,
    names_to = "visit", 
    names_prefix = "bdi_score_",
    values_to = "bdi")  %>% 
  select(id, visit, everything())  %>% 
  mutate(
    visit = replace(visit, visit == "bl", "00m"),
    visit = factor(visit, levels = str_c(c("00", "01", "06", "12"), "m"))) %>% 
  arrange(id, visit)

pulse_data %>% 
  group_by(visit) %>%  
  summarize(
    mean_bdi = mean(bdi, na.rm = TRUE),
    median_bdi = median(bdi, na.rm = TRUE)) %>%  
  knitr::kable(digits = 3)
```

In the FAS data, there are several outcomes of interest; for now, focus on post-natal day on which a pup is able to pivot. Two predictors of interest are the dose level and the day of treatment. Produce a reader-friendly table that quantifies the possible associations between dose, day of treatment, and the ability to pivot.

```{r}
litters_df = 
  read_csv("data/FAS_litters.csv", na = c("NA", ".", "")) %>% 
  janitor::clean_names() %>% 
  separate(
    group, into = c("dose", "tx_day"), sep = 3
  )

pups_df = 
  read_csv("data/FAS_pups.csv", na = c("NA", ".", "")) %>% 
  janitor::clean_names()

fas_df = 
  left_join(pups_df, litters_df, by = "litter_number")
```

Now I have all the information that I need
Now to compute

```{r}
fas_df %>% 
  drop_na(dose) %>% 
  group_by(dose, tx_day) %>% 
  summarise(mean_pivot = mean(pd_pivot, na.rm = TRUE)) %>% 
  pivot_wider(
    names_from = tx_day,
    values_from = mean_pivot
  ) %>% 
  knitr::kable(digits = 2)
```

